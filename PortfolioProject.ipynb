{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voxg2txsdor1",
        "outputId": "1617b13f-0c7c-42f9-a7a0-c28779f9875a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# DATABASE BUILDING\n",
        "1. Grabbing the top 100 companies\n",
        "2. Downloading those to a csv to save it\n",
        "3. From that list, getting the investor relations page from their website to download the last 3-4 quarters of reports, or 2023 year\n",
        "4. Connecting and reading from the folder containing all the reports in pdf format\n",
        "5. Download the top 100 companies past-year historical data (2023) and store in drive\n",
        "6. Grab data in github from 2015-2016, both the stock data as well as press releases\n",
        "\n",
        "-----------------------\n",
        "#MODEL - Text\n",
        "1. Build a BERT model\n",
        "2. Train said BERT model on the press releases from 2023 and 2015\n",
        "3. Cluster on the embeddings using DBSCAN\n",
        "4. Interpret clusters\n",
        "5. Compare and contrast the differences?\n",
        "6. Analyze & predict!\n",
        "\n",
        "#MODEL - Stocks\n",
        "1. Build time series model\n",
        "2. Random forest, XGboost, LSTM(Long Short-Term Memory) Network, Neural Nets?\n",
        "3. Analyze and optimize\n",
        "4. Predict and compare\n",
        "\n",
        "-----------------------\n",
        "\n",
        "#My github repo! (Bread & butter to this project)\n",
        "https://github.com/kkhawk20/MGSC310-Final-Project"
      ],
      "metadata": {
        "id": "i-RF9q5_FCtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATABASE BUILDING"
      ],
      "metadata": {
        "id": "0rQjzPE5eysq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Grabbing the top companies off Yahoo Finance\n",
        "#This was run on 11/13/23, stock data came from this day!\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Function to get most active symbols from Yahoo Finance\n",
        "def get_most_active_symbols(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "    symbols = []\n",
        "    for row in soup.select('tr.simpTblRow'):\n",
        "        symbol = row.select_one('td:nth-of-type(1) a').text.strip()\n",
        "        symbols.append(symbol)\n",
        "\n",
        "    return symbols\n",
        "\n",
        "most_active_symbols = get_most_active_symbols('https://finance.yahoo.com/most-active')"
      ],
      "metadata": {
        "id": "XQfUTRTceMQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Download the top companies and save it\n",
        "# DO NOT RUN THIS UNLESS YOU WANT TO DOWNLOAD THE CSV\n",
        "\n",
        "import csv\n",
        "\n",
        "#list of top 100 companies\n",
        "your_list = most_active_symbols\n",
        "\n",
        "# Open a new CSV file\n",
        "with open('stock_tickers.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Writing each item in the list as a new row\n",
        "    for item in your_list:\n",
        "        writer.writerow([item])\n",
        "\n",
        "# Download the file to your local machine (specific to Google Colab)\n",
        "from google.colab import files\n",
        "files.download('stock_tickers.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "VaomrAiREaYa",
        "outputId": "3f2d40d8-422a-47e2-e059-fc394f6b44e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b25d70d1-64c5-4b70-8870-d213aae8247d\", \"stock_tickers.csv\", 137)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Grabbing all the investor relations websites and exporting to a CSV file with the subsequent Ticker\n",
        "# I will now go through each website and download each of the files into a shared google doc\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file containing the stock tickers\n",
        "file_path = 'https://raw.githubusercontent.com/kkhawk20/MGSC310-Final-Project/main/Market_data.csv'\n",
        "stock_tickers = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the contents\n",
        "stock_tickers.head()\n",
        "\n",
        "# Function to find investor relations website for a given stock ticker\n",
        "def find_investor_relations_website(ticker):\n",
        "    # Constructing the Google search query\n",
        "    query = f\"{ticker} investor relations site\"\n",
        "\n",
        "    try:\n",
        "        # Performing the search\n",
        "        search_result = requests.get(f\"https://www.google.com/search?q={query}\")\n",
        "        soup = BeautifulSoup(search_result.text, 'html.parser')\n",
        "\n",
        "        # Finding all the links in the search result\n",
        "        links = soup.find_all('a')\n",
        "\n",
        "        for link in links:\n",
        "            # Extracting the href attribute of each link\n",
        "            href = link.get('href')\n",
        "\n",
        "            # Looking for the first valid URL which is typically the company's investor relations page\n",
        "            if \"url?q=\" in href and not \"webcache.googleusercontent.com\" in href:\n",
        "                url = re.findall(\"url\\?q=(.*?)&\", href)\n",
        "                if url:\n",
        "                    return url[0]\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Testing the function with a few tickers from the list\n",
        "all_tickers = stock_tickers['Symbol']\n",
        "investor_relations_websites = {ticker: find_investor_relations_website(ticker) for ticker in all_tickers}\n",
        "\n",
        "# Open a new CSV file to download\n",
        "with open('investor_relations_website.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    # Writing the header\n",
        "    writer.writerow(['Ticker', 'Investor Relations Website'])\n",
        "    # Writing each ticker and its corresponding URL as a new row\n",
        "    for ticker, url in investor_relations_websites.items():\n",
        "        writer.writerow([ticker, url])\n",
        "\n",
        "# Download the file to your local\n",
        "from google.colab import files\n",
        "files.download('investor_relations_website.csv')"
      ],
      "metadata": {
        "id": "TaxLmk6DKymc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Connecting and reading from the folder containing all the reports in pdf format\n",
        "\n",
        "import pdfplumber\n",
        "import os\n",
        "\n",
        "# Path to your folder in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/MGSC 310/Portfolio Project/Shareholder Reports'\n",
        "\n",
        "# Loop through all files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text = page.extract_text()\n",
        "                # print(f\"Contents of {filename}:\")\n",
        "                # print(text)\n"
      ],
      "metadata": {
        "id": "SknPCFFmaLn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Downloading the past-year historical data for each of the 100 companies\n",
        "#DO NOT RUN THIS AGAIN DEAR GOD DO NOT RUN THIS EVER AGAIN\n",
        "#This data was pulled on 11/14/23\n",
        "\n",
        "''' Safeguards to not run it again\n",
        "\n",
        "import os\n",
        "import yfinance as yf\n",
        "\n",
        "# Define the list of stock tickers\n",
        "stock_tickers_data = pd.read_csv('https://raw.githubusercontent.com/kkhawk20/MGSC310-Final-Project/main/Market_data.csv')\n",
        "stock_tickers = stock_tickers_data['Symbol'].tolist()\n",
        "\n",
        "# Path to the folder in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/MGSC 310/Portfolio Project/Yahoo Finance Stock Data'\n",
        "\n",
        "# Check if the folder exists, if not create it\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Loop through each ticker, fetch data, and save as CSV\n",
        "for ticker in stock_tickers:\n",
        "    stock_data = yf.download(ticker, period='1y')  # Download past year data\n",
        "    file_path = os.path.join(folder_path, f'{ticker}_data.csv')\n",
        "    stock_data.to_csv(file_path)\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "e2_lWIPzaTGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Grab data in github from 2015-2016, both the stock data as well as press releases"
      ],
      "metadata": {
        "id": "W4KAk-crdR2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL"
      ],
      "metadata": {
        "id": "S0qaOLY9e1yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenize\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "input_text = \"We really very much value AI\"\n",
        "encoded_input = tokenizer(input_text, return_tensors='pt')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model(**encoded_input)\n",
        "\n",
        "  last_hidden_states = output.last_hidden_state\n",
        "  embeddings = last_hidden_states[0]"
      ],
      "metadata": {
        "id": "72W7msflOKto"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}